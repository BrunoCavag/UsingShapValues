{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP Force Plots for Classification\n",
    "\n",
    "Pretty much all examples of Force Plots have been use for continious or binary classification. You actually can produce force plots for multi-class targets, it just takes a litle extra digging. \n",
    "\n",
    "To sum up very briefly, shapley values give us a metric for evaluating the importance of a predictor relative to other predictors. Essentially these values take into account how the loss function (model error) is affected by knowing vs. not knowing about the predictive feature. These values also indicate the direction of the relationship (positive or negative) between the predictive feature and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have succesfully imported SHAP, one of the visualizations you can produce is the force plot. Force plots, allows you to see how features contributed to the model prediction for a specific observation. \n",
    "\n",
    "However, the plot is only the output you get from using *shap.force_plot()*. It doesn't tell you the predicted output of the model, nor does it tell you the ground truth label for this specific observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force Plots for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (train_test_split, OneHotEncoder)\n",
    "from sklearn.pipeline import (Pipeline,)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "## Import X and Y:\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "random_seed = 1234\n",
    "\n",
    "\n",
    "## Split the data into training and test sets prior to preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=random_seed)\n",
    "\n",
    "## Make a list of all columns that are currently object dtype\n",
    "cat_cols = list(X_train.select_dtypes('O').columns)\n",
    "\n",
    "## Create a pipeline for one hot encoding categorical columns\n",
    "cat_transformer = Pipeline(steps = [\n",
    "  ('ohe', OneHotEncoder(handle_unknown='error', \n",
    "                        sparse=False,\n",
    "                        drop='if_binary'))])\n",
    "\n",
    "## Define pipeline for preprocessing X\n",
    "preprocessing = ColumnTransformer(transformers=[\n",
    "    ('cat', cat_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "## Preprocess training and test predictors (X)\n",
    "X_train_tf = preprocessing.fit_transform(X_train)\n",
    "X_test_tf = preprocessing.transform(X_test)\n",
    "\n",
    "## Get the feature names in the order they appear in preprocessed data\n",
    "feature_names = preprocessing.named_transformers_['cat'].named_steps['ohe'].get_feature_names(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training a model: \"binary_model\"\n",
    "\n",
    "## Calculate SHAP values for model\n",
    "binary_explainer = shap.TreeExplainer(binary_model)\n",
    "binary_shap_values = binary_explainer.shap_values(X_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: I have seen some functions for producing visualizations that include the calculation of SHAP values as part of the function. I do not recommend doing this. Depending on the size of your dataset, the calculation can take a decent amount of time and there is no reason to do this each time you want to produce a plot for the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_force(clf, clf_step_name, index, \n",
    "               X_train_df, y_train,\n",
    "               explainer, shap_vals):\n",
    "  \n",
    "    \"\"\"Takes in a fitted classifier Pipeline, the name of the classifier step,\n",
    "        the X training DataFrame, the y train array, a shap explainer, and the\n",
    "        shap values to print the ground truth and predicted label and display\n",
    "        the shap force plot for the record specified by index.\n",
    "    Args:\n",
    "        clf (estimator): An sklearn Pipeline with a fitted classifier as the final step.\n",
    "        clf_step_name (str): The name given to the classifier step of the pipe.\n",
    "        X_train_df (DataFrame): A Pandas DataFrame from the train-test-split\n",
    "            used to train the classifier, with column names corresponding to\n",
    "            the feature names.\n",
    "        y_train (series or array): Subset of y data used for training.\n",
    "        index (int): The index of the observation of interest.\n",
    "        explainer (shap explainer): A fitted shap.TreeExplainer object.\n",
    "        shap_vals (array): The array of shap values.\n",
    "    Returns:\n",
    "        Figure: Shap force plot showing the breakdown of how the model made\n",
    "            its prediction for the specified record in the training set.\n",
    "    \"\"\"    \n",
    "    \n",
    "    \n",
    "    ## Store model prediction and ground truth label\n",
    "    pred = clf.named_steps[clf_step_name].predict(X_train_df.iloc[index,:])\n",
    "    true_label = y_train.iloc[index]\n",
    "    \n",
    "    \n",
    "    ## Assess accuracy of prediction\n",
    "    if true_label == pred:\n",
    "        accurate = 'Correct!'\n",
    "    else:\n",
    "        accurate = 'Incorrect'\n",
    "    \n",
    "    \n",
    "    ## Print output that checks model's prediction against true label\n",
    "    print('***'*12)\n",
    "    # Print ground truth label for row at index\n",
    "    print(f'Ground Truth Label: {true_label}')\n",
    "    print()\n",
    "    # Print model prediction for row at index\n",
    "    print(f'Model Prediction:  {pred} -- {accurate}')\n",
    "    print('***'*12)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    ## Plot the prediction's explanation\n",
    "    fig = shap.force_plot(explainer.expected_value,\n",
    "                              shap_vals[index,:],\n",
    "                              X_train_df.iloc[index,:])\n",
    "    \n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print model prediction, true label, and shap force plot for third row\n",
    "  ## of training set\n",
    "\n",
    "shap_force(binary_est, \n",
    "           'xgb', # name of fitted classifier step in pipeline\n",
    "           2, X_train_df, y_train, \n",
    "           binary_explainer, \n",
    "           binary_shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force Plots for Multi-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## Preprocess training and test target (y) after having performed train-test split\n",
    "le = LabelEncoder()\n",
    "y_multi_train = pd.Series(le.fit_transform(y_multi_train))\n",
    "y_multi_test = pd.Series(le.transform(y_multi_test))\n",
    "\n",
    "## Check classes\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Store the best fitted classifier and its booster\n",
    "multi_est = xgb_multi_grid.best_estimator_ # from tuning with GridSearchCV\n",
    "multi_model = multi_est.named_steps['xgb'].get_booster()\n",
    "\n",
    "## Convert transformed (preprocessed) X train set into pandas DataFrame\n",
    "X_train_df = pd.DataFrame(X_train_tf, columns=feature_names)\n",
    "\n",
    "## Calculate SHAP values for model\n",
    "multi_explainer = shap.TreeExplainer(multi_model)\n",
    "multi_shap_values = multi_explainer.shap_values(X_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the type of the output when calculating SHAP values for a multi-class target\n",
    "print(type(multi_shap_values)) # output: <class 'list'>\n",
    "\n",
    "## Check the type of the first item in the list\n",
    "print(type(multi_shap_values[0])) # output: <class 'numpy.ndarray'>\n",
    "\n",
    "## Check the number of items in the list\n",
    "print(len(multi_shap_values)) # output: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_shap_force(clf, clf_step_name, index,\n",
    "                     X_train_df, y_train,\n",
    "                     explainer, multi_shap_vals,\n",
    "                     classes='all'):\n",
    "  \n",
    "    \"\"\"Takes in a fitted classifier Pipeline, the name of the classifier step,\n",
    "        the X training DataFrame, the y train array, a shap explainer, and the\n",
    "        multiclass shap values. Prints the ground truth and predicted label for\n",
    "        the record of interest and displays shap force plots of the desired classes\n",
    "        for the record specified by index.\n",
    "    Args:\n",
    "        clf (estimator): An sklearn Pipeline with a fitted classifier as the final step.\n",
    "        clf_step_name (str): The name given to the classifier step of the pipe.\n",
    "        index (int): The index of the observation of interest.\n",
    "        X_train_df (DataFrame): A Pandas DataFrame that from the train-test-split\n",
    "            used to train the classifier, with column names corresponding to\n",
    "            the feature names.\n",
    "        y_train (series or array): Subset of y data used for training.\n",
    "        explainer (shap explainer): A fitted shap.TreeExplainer object.\n",
    "        multi_shap_vals (list): The list of arrays of shap values. One array per \n",
    "            target label.\n",
    "        classes (str, optional): A string specifying which shap force plots\n",
    "            to display for the specified record. Options are 'all' (displays for all\n",
    "            class labels), 'true' (displays only the plot for the ground truth label for\n",
    "            the record), 'pred' (displays only the plot for the predicted label for\n",
    "            the record), or 'both' (displays both 'true' and 'pred'). Defaults to 'all'.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Create dict for mapping class labels\n",
    "    label_dict = {0: 'Early',\n",
    "                  1: 'Election Day',\n",
    "                  2: 'No Vote'}\n",
    "\n",
    "    \n",
    "    ## Store model prediction and ground truth label for specified index\n",
    "    pred = int(clf.named_steps[clf_step_name].predict(X_train_df.iloc[index,:]))\n",
    "    true_label = pd.Series(y_train).iloc[index]\n",
    "\n",
    "\n",
    "    ## Assess accuracy of prediction\n",
    "    if true_label == pred:\n",
    "        accurate = 'Correct!'\n",
    "    else:\n",
    "        accurate = 'Incorrect'\n",
    "        \n",
    "\n",
    "    ## Print output that checks model's prediction against true label\n",
    "    print('***'*17)\n",
    "    # Print ground truth label for row at index\n",
    "    print(f'Ground Truth Label: {true_label} - {label_dict[true_label]}')\n",
    "    print()\n",
    "    # Print model prediction for row at index\n",
    "    print(f'Model Prediction:  [{pred}] - {label_dict[pred]} -- {accurate}')\n",
    "    print('***'*17)\n",
    "    print()\n",
    "    print()\n",
    " \n",
    "    \n",
    "    ## Determine which classes to show force plots for\n",
    "    # All classes \n",
    "    if classes == 'all':\n",
    "        ## Visualize the ith prediction's explanation for all classes\n",
    "        print('Early Vote Class (0)')\n",
    "        display(shap.force_plot(explainer.expected_value[0],\n",
    "                    multi_shap_vals[0][index],\n",
    "                    X_train_df.iloc[index,:]))\n",
    "        print()\n",
    "\n",
    "        print('Election Day Vote Class (1)')\n",
    "        display(shap.force_plot(explainer.expected_value[1],\n",
    "                    multi_shap_vals[1][index],\n",
    "                    X_train_df.iloc[index,:]))\n",
    "        print()\n",
    "\n",
    "        print('No Vote Class (2)')\n",
    "        display(shap.force_plot(explainer.expected_value[2],\n",
    "                    multi_shap_vals[2][index],\n",
    "                    X_train_df.iloc[index,:]))\n",
    "        \n",
    "    \n",
    "    # Only the class predicted by the model\n",
    "    elif classes == 'pred':\n",
    "        print(f'Predicted: {label_dict[pred]} Class {pred}')\n",
    "        display(shap.force_plot(explainer.expected_value[pred],\n",
    "                                multi_shap_vals[pred][index],\n",
    "                                X_train_df.iloc[index,:]))\n",
    "    \n",
    "    \n",
    "    # Only the ground truth label\n",
    "    elif classes == 'true':\n",
    "        print(f'True: {label_dict[true_label]} Class {true_label}')\n",
    "        display(shap.force_plot(explainer.expected_value[true_label],\n",
    "                    multi_shap_vals[true_label][index],\n",
    "                    X_train_df.iloc[index,:]))\n",
    "    \n",
    "    \n",
    "    # Both the predicted and ground truth (identical plots if prediction is correct)\n",
    "    elif classes == 'both':\n",
    "        print(f'Predicted: {label_dict[pred]} Class {pred}')\n",
    "        display(shap.force_plot(explainer.expected_value[pred],\n",
    "                                multi_shap_vals[pred][index],\n",
    "                                X_train_df.iloc[index,:]))\n",
    "        print()\n",
    "\n",
    "        print(f'True: {label_dict[true_label]} Class {true_label}')\n",
    "        display(shap.force_plot(explainer.expected_value[true_label],\n",
    "                    multi_shap_vals[true_label][index],\n",
    "                    X_train_df.iloc[index,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_shap_force_le(clf, clf_step_name, index,\n",
    "                        X_train_df, y_train,\n",
    "                        explainer, multi_shap_vals,\n",
    "                        le_classes,\n",
    "                        classes='both'):\n",
    "  \n",
    "    \"\"\"Takes in a fitted classifier Pipeline, the name of the classifier step,\n",
    "        the X training DataFrame, the y train array, a shap explainer, and the\n",
    "        multiclass shap values to print the ground truth and predicted label for\n",
    "        the record and display shap force plots of the desired classes\n",
    "        for the record specified by index.\n",
    "    Args:\n",
    "        clf (estimator): An sklearn Pipeline with a fitted classifier as the final step.\n",
    "        clf_step_name (str): The name given to the classifier step of the pipe.\n",
    "        index (int): The index of the observation of interest.\n",
    "        X_train_df (DataFrame): A Pandas DataFrame that from the train-test-split\n",
    "            used to train the classifier, with column names corresponding to\n",
    "            the feature names.\n",
    "        y_train (series or array): Subset of y data used for training.\n",
    "        explainer (shap explainer): A fitted shap.TreeExplainer object\n",
    "        multi_shap_vals (list): The list of arrays of shap values. One array per \n",
    "            target label.\n",
    "        le_classes (array): The classes_ attribute of the label encoded target variable.\n",
    "        classes (str, optional): A string specifying which shap force plots\n",
    "            to display for the specified record. Options are 'all' (displays for all\n",
    "            class labels), 'true' (displays only the plot for the ground truth label for\n",
    "            the record), 'pred' (displays only the plot for the predicted label for\n",
    "            the record), or 'both' (displays both 'true' and 'pred'). Defaults to 'both'.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Create dict for mapping class labels\n",
    "    label_dict = {}\n",
    "    for i, label in list(enumerate(le_classes)):\n",
    "        label_dict[i] = label\n",
    "        \n",
    "    ## Store model prediction and ground truth label for that index\n",
    "    pred = int(clf.named_steps[clf_step_name].predict(X_train_df.iloc[index,:]))\n",
    "    true_label = pd.Series(y_train).iloc[index]\n",
    "\n",
    "\n",
    "    ## Assess accuracy of prediction\n",
    "    if true_label == pred:\n",
    "        accurate = 'Correct!'\n",
    "    else:\n",
    "        accurate = 'Incorrect'\n",
    "        \n",
    "\n",
    "    ## Print output that checks model's prediction against true label\n",
    "    print('***'*17)\n",
    "    # Print ground truth label for row at index\n",
    "    print(f'Ground Truth Label: {true_label} - {label_dict[true_label]}')\n",
    "    print()\n",
    "    # Print model prediction for row at index\n",
    "    print(f'Model Prediction:  [{pred}] - {label_dict[pred]} -- {accurate}')\n",
    "    print('***'*17)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    " \n",
    "    ## Determine which classes to show force plots for\n",
    "    # All classes \n",
    "    if classes == 'all':\n",
    "        ## Visualize the ith prediction's explanation for all classes\n",
    "        for key in range(len(label_dict)):\n",
    "            print(f'{label_dict[key]} Class ({key})')\n",
    "            display(shap.force_plot(explainer.expected_value[key],\n",
    "                        multi_shap_vals[key][index],\n",
    "                        X_train_df.iloc[index,:]))\n",
    "            print()\n",
    "    \n",
    "    \n",
    "    # Only the class predicted by the model\n",
    "    elif classes == 'pred':\n",
    "        print(f'Predicted: {label_dict[pred]} Class {pred}')\n",
    "        display(shap.force_plot(explainer.expected_value[pred],\n",
    "                                multi_shap_vals[pred][index],\n",
    "                                X_train_df.iloc[index,:]))\n",
    "\n",
    "     \n",
    "    # Only the ground truth label\n",
    "    elif classes == 'true':\n",
    "        print(f'True: {label_dict[true_label]} Class {true_label}')\n",
    "        display(shap.force_plot(explainer.expected_value[true_label],\n",
    "                    multi_shap_vals[true_label][index],\n",
    "                    X_train_df.iloc[index,:]))\n",
    "\n",
    "    \n",
    "    # Both the predicted and ground truth (identical plots if prediction is correct)\n",
    "    elif classes == 'both':\n",
    "        print(f'Predicted: {label_dict[pred]} Class {pred}')\n",
    "        display(shap.force_plot(explainer.expected_value[pred],\n",
    "                                multi_shap_vals[pred][index],\n",
    "                                X_train_df.iloc[index,:]))\n",
    "        print()\n",
    "\n",
    "        print(f'True: {label_dict[true_label]} Class {true_label}')\n",
    "        display(shap.force_plot(explainer.expected_value[true_label],\n",
    "                    multi_shap_vals[true_label][index],\n",
    "                    X_train_df.iloc[index,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_shap_force(multi_est, 'xgb', 2,\n",
    "                 X_train_df, y_train,\n",
    "                 multi_explainer,\n",
    "                 multi_shap_values,\n",
    "                 classes='all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('bru_ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba7033ad42021d8fde750941f7374709dc7fad13ee27a9cd8f3ab36e378bdd7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
